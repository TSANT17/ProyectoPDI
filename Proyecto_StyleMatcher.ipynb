{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSANT17/ProyectoPDI/blob/main/Proyecto_StyleMatcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Santiago Parrado\n",
        "Carlos Lopez\n",
        "Sebastian Vidal\n",
        "Hernan Gutierrez"
      ],
      "metadata": {
        "id": "uBWyR-JAh8jV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZpDDD6YWeF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fb7500-569c-4941-ce8a-eacb7587e1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (10.2.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe\n",
        "!pip install facenet-pytorch\n",
        "!pip install gradio\n",
        "!pip install torch torchvision\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLctUFezx1fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26082831-f63f-4f13-bc74-d017785bcef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy('/content/drive/MyDrive/StyleMatcher/model_anime_best.pt', '/content/model_anime_best.pt')\n",
        "shutil.copy('/content/drive/MyDrive/StyleMatcher/model_painting_best.pt', '/content/model_painting_best.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "swtl9VqWKqhc",
        "outputId": "6b463204-75eb-4e50-f420-7719971afb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/model_painting_best.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8Qokvj4YppG"
      },
      "source": [
        "**Normalizar todas las categorias**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "758NR4TvYIzO",
        "outputId": "35dc3271-2b34-4cc8-b9b0-ee9435c287c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Redimensionando /content/drive/My Drive/StyleMatcher/dataset/train/anime:  62%|██████▏   | 590/950 [15:27<09:49,  1.64s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# Ruta de entrada y salida\n",
        "root_input = '/content/drive/My Drive/StyleMatcher/dataset'\n",
        "root_output = '/content/drive/My Drive/StyleMatcher/dataset_resized'\n",
        "target_size = (512, 512)\n",
        "\n",
        "def resize_and_save(input_path, output_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    img_paths = glob(os.path.join(input_path, '*'))\n",
        "\n",
        "    for img_path in tqdm(img_paths, desc=f'Redimensionando {input_path}'):\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            resized = cv2.resize(img, target_size)\n",
        "            name = os.path.basename(img_path)\n",
        "            cv2.imwrite(os.path.join(output_path, name), resized)\n",
        "        except Exception as e:\n",
        "            print(f\"Error con {img_path}: {e}\")\n",
        "\n",
        "def process_dataset(root_input, root_output):\n",
        "    for phase in ['train', 'val']:\n",
        "        phase_path = os.path.join(root_input, phase)\n",
        "        for class_name in os.listdir(phase_path):\n",
        "            input_folder = os.path.join(phase_path, class_name)\n",
        "            output_folder = os.path.join(root_output, phase, class_name)\n",
        "            resize_and_save(input_folder, output_folder)\n",
        "\n",
        "# Ejecutamos\n",
        "process_dataset(root_input, root_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports Y configuración inicial"
      ],
      "metadata": {
        "id": "w4oa5JlEvQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torchvision import models, transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "import mediapipe as mp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "wMUIxJiQvQcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo de estilo con VGG19 multicapa"
      ],
      "metadata": {
        "id": "TDFK3d2dvavJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG19StyleExtractor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features\n",
        "        self.layers = torch.nn.Sequential(*[vgg[i] for i in [0, 5, 10, 19, 28]])  # conv1_1 a conv5_1\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            features.append(torch.nn.functional.adaptive_avg_pool2d(x, (1, 1)))\n",
        "        return torch.cat([f.view(f.size(0), -1) for f in features], dim=1)\n",
        "\n",
        "# Instanciar el modelo\n",
        "vgg_model = VGG19StyleExtractor().to(device)"
      ],
      "metadata": {
        "id": "3dzNBP3pvcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39f2ba0-3145-4848-bb77-b69754f0d8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 194MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo de identidad con FaceNet"
      ],
      "metadata": {
        "id": "k_Ftkw1Pvg9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n"
      ],
      "metadata": {
        "id": "wTDSJQrWvjVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9a2d6d4827c94d09bd4f3dba39701d93",
            "27d16ae78c0b40db82b554071767b746",
            "63872ff8e1234b418f17e7777676989b",
            "fda80a3f3ca846228c415c678b13476f",
            "d54305ccecab4187952c032b36483762",
            "bb761c9fe04e42aeb85c880a513a4e84",
            "7696fb347eb447b5b10c654b02c32b36",
            "288a297c30f2478b926f033aefdbb558",
            "4d6baa0739e54cf38320f9ca046d1547",
            "5603697fabbb4215b4a4aa86fef9cad3",
            "bee803f1778147238c40b59e6d4417b3"
          ]
        },
        "outputId": "e15318c3-15bb-467b-a729-2d984d34b3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a2d6d4827c94d09bd4f3dba39701d93"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detector de pose con MediaPipe"
      ],
      "metadata": {
        "id": "5fctLksFvlHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "pose_estimator = mp_pose.Pose(static_image_mode=True)"
      ],
      "metadata": {
        "id": "cwQfqQ8Yvng0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transforms para VGG y FaceNet"
      ],
      "metadata": {
        "id": "Yc0-9f2NvsVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "facenet_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "lo9iZ8zFvuQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Función para extraer todos los vectores de una imagen"
      ],
      "metadata": {
        "id": "K0_mgfuZvwVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_features(img_path):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        return None, None, None\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_vgg = vgg_transform(img_rgb).unsqueeze(0).to(device)\n",
        "    img_facenet = facenet_transform(img_rgb).unsqueeze(0).to(device)\n",
        "\n",
        "    # Estilo\n",
        "    with torch.no_grad():\n",
        "        style_vec = vgg_model(img_vgg).cpu().numpy().flatten()\n",
        "\n",
        "    # Identidad\n",
        "    with torch.no_grad():\n",
        "        identity_vec = identity_model(img_facenet).cpu().numpy().flatten()\n",
        "\n",
        "    # Pose\n",
        "    results = pose_estimator.process(img_rgb)\n",
        "    pose_map = np.zeros((256, 256), dtype=np.uint8)\n",
        "    if results.pose_landmarks:\n",
        "        h, w = 256, 256\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            x = int(lm.x * w)\n",
        "            y = int(lm.y * h)\n",
        "            if 0 <= x < w and 0 <= y < h:\n",
        "                pose_map[y, x] = 255\n",
        "\n",
        "    return style_vec, identity_vec, pose_map"
      ],
      "metadata": {
        "id": "GsdMeGbQvyRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Función para procesar una carpeta completa"
      ],
      "metadata": {
        "id": "ibYm7m7uv5q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_folder(image_folder, style_out, input_out):\n",
        "    os.makedirs(style_out, exist_ok=True)\n",
        "    os.makedirs(input_out, exist_ok=True)\n",
        "    img_paths = glob(os.path.join(image_folder, '*'))\n",
        "\n",
        "    for img_path in tqdm(img_paths, desc=f\"Procesando {image_folder}\"):\n",
        "        name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        style_vec, identity_vec, pose_map = extract_all_features(img_path)\n",
        "        if style_vec is None:\n",
        "            continue\n",
        "        np.save(os.path.join(style_out, f\"{name}_style.npy\"), style_vec)\n",
        "        np.save(os.path.join(input_out, f\"{name}_identity.npy\"), identity_vec)\n",
        "        cv2.imwrite(os.path.join(input_out, f\"{name}_pose.png\"), pose_map)"
      ],
      "metadata": {
        "id": "vdN2OEH_v6uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejecutar el extractor sobre tus datasets"
      ],
      "metadata": {
        "id": "eyDseenvv-Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    \"anime\": {\n",
        "        \"input\": \"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "        \"style\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "        \"others\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    },\n",
        "    \"painting\": {\n",
        "        \"input\": \"/content/drive/My Drive/StyleMatcher/dataset/train/painting\",\n",
        "        \"style\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_styles\",\n",
        "        \"others\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_inputs\",\n",
        "    }\n",
        "}\n",
        "\n",
        "for key, paths in datasets.items():\n",
        "    process_folder(paths[\"input\"], paths[\"style\"], paths[\"others\"])"
      ],
      "metadata": {
        "id": "INeGz5CEwB-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea8d305-5817-4f8e-e8f8-a849d41c00c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando /content/drive/My Drive/StyleMatcher/dataset/train/anime: 100%|██████████| 950/950 [15:11<00:00,  1.04it/s]\n",
            "Procesando /content/drive/My Drive/StyleMatcher/dataset/train/painting: 100%|██████████| 750/750 [11:33<00:00,  1.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkkHimRwnRgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "85c8f8f2-cd77-4c83-8cf9-4632876377fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_and_save_pose' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-469af7caed0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m extract_and_save_pose(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/StyleMatcher/input/pose 1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msave_image_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/StyleMatcher/input/pose1_pose.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msave_json_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/StyleMatcher/input/pose1_pose.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_and_save_pose' is not defined"
          ]
        }
      ],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 1.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose1_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose1_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6RncCZ7nlIe"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 2.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose2_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose2_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0d7UZGNnpe7"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 3.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose3_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose3_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vf0PONVIRI5"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 4.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose4_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose4_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpCcgLvUJ5pR"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 5.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose5_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose5_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0KxhpycN4HA"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 6.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose6_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose6_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wDKawSnO32n"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 7.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose7_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose7_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NfWz6zVO6EE"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 8.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose8_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose8_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6FqeaMdPIJk"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 9.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose9_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose9_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACXObIL3PxmC"
      },
      "outputs": [],
      "source": [
        "extract_and_save_pose(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/pose 10.jpg',\n",
        "    save_image_path='/content/drive/My Drive/StyleMatcher/input/pose10_pose.jpg',\n",
        "    save_json_path='/content/drive/My Drive/StyleMatcher/input/pose10_pose.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8zRTjgnTujz"
      },
      "outputs": [],
      "source": [
        "extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo1.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/test_01_style.npy',\n",
        "    layer=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lFbxuRWT-kE"
      },
      "source": [
        "**deberia** aparecer un archivo .npy que contiene las matrices de Gram para las capas seleccionadas. Esta será tu “huella digital” del estilo, y luego se usará como guía para pintar la imagen generada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_W8-yGsUHO7"
      },
      "source": [
        "**Recomendaciones:**\n",
        "Para cada estilo (anime, cartoon, pintura), selecciona unas 5–10 imágenes base representativas.\n",
        "\n",
        "Puedes extraer sus estilos y tener un banco de estilos preprocesado.\n",
        "\n",
        "Esto será útil si luego quieres permitir seleccionar estilo desde una galería sin tener que volver a calcularlo.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Despues de mejorar el codigo ya se puede seguir con el paso 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEjQUt0LDOrF"
      },
      "outputs": [],
      "source": [
        "style_features = extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo2.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/estilo2_style.npy'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHHSZL9Tjjfx"
      },
      "outputs": [],
      "source": [
        "style_features = extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo3.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/estilo3_style.npy'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H8_EQzbjxFH"
      },
      "outputs": [],
      "source": [
        "style_features = extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo4.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/estilo4_style.npy'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5xQ36wAj0AM"
      },
      "outputs": [],
      "source": [
        "style_features = extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo5.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/estilo5_style.npy'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W_Y6CUtj3E7"
      },
      "outputs": [],
      "source": [
        "style_features = extract_style_features(\n",
        "    image_path='/content/drive/My Drive/StyleMatcher/input/estilo6.jpg',\n",
        "    save_path='/content/drive/My Drive/StyleMatcher/input/estilo6_style.npy'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjrxOwBOEman"
      },
      "source": [
        "#StylizerDatasetV3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importaciones"
      ],
      "metadata": {
        "id": "L6Ac1OiDzoEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "e41XSZ7lzqM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definición del Dataset"
      ],
      "metadata": {
        "id": "dBTgGn_3zugJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StylizerDatasetV3(Dataset):\n",
        "    def __init__(self, image_dir, style_dir, input_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.style_dir = style_dir\n",
        "        self.input_dir = input_dir\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.samples = self._gather_valid_samples()\n",
        "\n",
        "    def _gather_valid_samples(self):\n",
        "        all_images = [f for f in os.listdir(self.image_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "        valid = []\n",
        "        for img_name in all_images:\n",
        "            base = os.path.splitext(img_name)[0]\n",
        "            pose_path = os.path.join(self.input_dir, f\"{base}_pose.png\")\n",
        "            id_path = os.path.join(self.input_dir, f\"{base}_identity.npy\")\n",
        "            style_path = os.path.join(self.style_dir, f\"{base}_style.npy\")\n",
        "            if os.path.exists(pose_path) and os.path.exists(id_path) and os.path.exists(style_path):\n",
        "                valid.append(base)\n",
        "        return valid\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base = self.samples[idx]\n",
        "\n",
        "        # Imagen\n",
        "        img_path = os.path.join(self.image_dir, f\"{base}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            img_path = os.path.join(self.image_dir, f\"{base}.png\")\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Pose\n",
        "        pose_path = os.path.join(self.input_dir, f\"{base}_pose.png\")\n",
        "        pose = Image.open(pose_path).convert(\"L\")\n",
        "        pose = transforms.Resize((256, 256))(pose)\n",
        "        pose = transforms.ToTensor()(pose)\n",
        "\n",
        "        # Identidad y estilo\n",
        "        identity = np.load(os.path.join(self.input_dir, f\"{base}_identity.npy\"))\n",
        "        style = np.load(os.path.join(self.style_dir, f\"{base}_style.npy\"))\n",
        "\n",
        "        # Forzamos tamaños seguros\n",
        "        identity = torch.tensor(identity[:512], dtype=torch.float32)       # Asegura 512\n",
        "        style = torch.tensor(style[:1472], dtype=torch.float32)           # Asegura 1472\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"pose\": pose,\n",
        "            \"identity\": identity,\n",
        "            \"style\": style\n",
        "        }"
      ],
      "metadata": {
        "id": "lh30Ltclzwh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZkiE1rlEojg"
      },
      "source": [
        "#Generador StylizerUNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNFqfqvalXKA"
      },
      "outputs": [],
      "source": [
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        if use_dropout:\n",
        "            self.block.add_module(\"dropout\", nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class StylizerUNet(nn.Module):\n",
        "    def __init__(self, id_dim=512, style_dim=1472):\n",
        "        super().__init__()\n",
        "        self.emb_dim = id_dim + style_dim  # 1984\n",
        "\n",
        "        self.embedding_expand = nn.Sequential(\n",
        "            nn.Linear(self.emb_dim, 512 * 4 * 4),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = UNetBlock(1, 64, down=True)    # 256 → 128\n",
        "        self.enc2 = UNetBlock(64, 128, down=True)  # 128 → 64\n",
        "        self.enc3 = UNetBlock(128, 256, down=True) # 64 → 32\n",
        "        self.enc4 = UNetBlock(256, 512, down=True) # 32 → 16\n",
        "        self.enc5 = UNetBlock(512, 512, down=True) # 16 → 8\n",
        "        self.enc6 = UNetBlock(512, 512, down=True) # 8 → 4\n",
        "        self.enc7 = UNetBlock(512, 512, down=True) # 4 → 2\n",
        "\n",
        "        # Bottleneck (embedding + encoder)\n",
        "        self.middle = nn.Sequential(\n",
        "            nn.Conv2d(512 + 512, 512, 3, padding=1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = UNetBlock(512, 512, down=False, use_dropout=True)        # 2 → 4\n",
        "        self.dec2 = UNetBlock(1024, 512, down=False, use_dropout=True)       # 4 → 8\n",
        "        self.dec3 = UNetBlock(1024, 512, down=False, use_dropout=True)       # 8 → 16\n",
        "        self.dec4 = UNetBlock(1024, 512, down=False)                         # 16 → 32\n",
        "        self.dec5 = UNetBlock(768, 256, down=False)                          # 32 → 64\n",
        "        self.dec6 = UNetBlock(384, 128, down=False)                          # 64 → 128\n",
        "        self.dec7 = UNetBlock(192, 64, down=False)                           # 128 → 256\n",
        "        self.final = nn.Conv2d(65, 3, kernel_size=3, padding=1)                      # 256 → 512\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, pose_map, identity_vec, style_vec):\n",
        "        emb = torch.cat([identity_vec, style_vec], dim=1)\n",
        "        emb = self.embedding_expand(emb).view(-1, 512, 4, 4)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(pose_map)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "        e6 = self.enc6(e5)\n",
        "        e7 = self.enc7(e6)\n",
        "\n",
        "        # Fusion\n",
        "        emb_up = F.interpolate(emb, size=e7.shape[2:])  # Garantiza matching\n",
        "        bottleneck = self.middle(torch.cat([e7, emb_up], dim=1))\n",
        "\n",
        "        # Decoder con skip connections\n",
        "        d1 = self.dec1(bottleneck)\n",
        "        d2 = self.dec2(torch.cat([d1, e6], dim=1))\n",
        "        d3 = self.dec3(torch.cat([d2, e5], dim=1))\n",
        "        d4 = self.dec4(torch.cat([d3, e4], dim=1))\n",
        "        d5 = self.dec5(torch.cat([d4, e3], dim=1))\n",
        "        d6 = self.dec6(torch.cat([d5, e2], dim=1))\n",
        "        d7 = self.dec7(torch.cat([d6, e1], dim=1))\n",
        "\n",
        "        # Ajustar tamaño del pose_map si es necesario\n",
        "        pose_resized = F.interpolate(pose_map, size=d7.shape[2:])\n",
        "        out = self.final(torch.cat([d7, pose_resized], dim=1))\n",
        "        return self.tanh(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo está listo para usarse en entrenamiento. Admite entradas:\n",
        "\n",
        "pose_map: tensor de forma [B, 1, 256, 256]\n",
        "\n",
        "identity_vec: tensor [B, 512]\n",
        "\n",
        "style_vec: tensor [B, 960]\n",
        "\n",
        "Y genera imágenes de salida [B, 3, 256, 256]."
      ],
      "metadata": {
        "id": "3x9bsC971MxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "gDAoGJoD1OOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importaciones y configuraciones"
      ],
      "metadata": {
        "id": "NpRmW5fJ1gSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "T8n7JKNc1ha0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cargar dataset y modelo"
      ],
      "metadata": {
        "id": "0sC0UDwN1jO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "anime_dataset = StylizerDatasetV3(\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\"\n",
        ")\n",
        "\n",
        "loader = DataLoader(anime_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Modelo\n",
        "model = StylizerUNet().to(device)"
      ],
      "metadata": {
        "id": "u6IosP721l0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Función de pérdida L1 + Perceptual"
      ],
      "metadata": {
        "id": "l52rvle_1yD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, layer_ids=[0, 5, 10]):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features[:max(layer_ids)+1].eval()\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg = vgg.to(device)\n",
        "        self.layers = layer_ids\n",
        "        self.criterion = nn.L1Loss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = 0\n",
        "        x = input\n",
        "        y = target\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            y = layer(y)\n",
        "            if i in self.layers:\n",
        "                loss += self.criterion(x, y)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "PWOmgoUD1zWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurar optimizador y pérdida"
      ],
      "metadata": {
        "id": "vyvX1PWp115w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_l1 = nn.L1Loss()\n",
        "criterion_perceptual = PerceptualLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "1J8RSQvt14o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loop de entrenamiento"
      ],
      "metadata": {
        "id": "t3WOTdJ117aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === OPTIMIZACIÓN GPU A100 ===\n",
        "torch.backends.cudnn.benchmark = True  # Acelera convoluciones dinámicas\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 30\n",
        "batch_size = 128  # A100 puede manejar incluso más si no hay OOM\n",
        "lr = 1e-4\n",
        "\n",
        "def train_model(category, image_dir, style_dir, input_dir, save_path):\n",
        "    print(f\"\\n🚀 Entrenando modelo para: {category.upper()} ({epochs} épocas)\")\n",
        "\n",
        "    dataset = StylizerDatasetV3(image_dir, style_dir, input_dir)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = StylizerUNet(id_dim=512, style_dim=1472).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    criterion = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"[{category}] Época {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            imgs = batch[\"image\"].to(device, non_blocking=True)\n",
        "            poses = batch[\"pose\"].to(device, non_blocking=True)\n",
        "            ids = batch[\"identity\"].to(device, non_blocking=True)\n",
        "            styles = batch[\"style\"].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(poses, ids, styles)\n",
        "\n",
        "            if outputs.shape != imgs.shape:\n",
        "                outputs = F.interpolate(outputs, size=imgs.shape[2:])\n",
        "\n",
        "            loss = criterion(outputs, imgs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(loader)\n",
        "\n",
        "        # Guardar el mejor modelo\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"💾 Nuevo mejor modelo guardado en {save_path} (loss={best_loss:.4f})\")\n",
        "\n",
        "    print(f\"✅ Entrenamiento completo de {category}. Mejor loss: {best_loss:.4f}\")\n",
        "\n",
        "# === ENTRENAMIENTO DE ANIME Y LUEGO PAINTING ===\n",
        "train_model(\n",
        "    category=\"anime\",\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    save_path=\"/content/drive/My Drive/StyleMatcher/model_anime_best.pt\"\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    category=\"painting\",\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/painting\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_inputs\",\n",
        "    save_path=\"/content/drive/My Drive/StyleMatcher/model_painting_best.pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "6UU61nGR2EtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bf2a42-ec01-4038-eb18-5434ae9e55bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Entrenando modelo para: ANIME (30 épocas)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 1/30: 100%|██████████| 8/8 [09:04<00:00, 68.11s/it, loss=0.142]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.2210)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 2/30: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, loss=0.0783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.1003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 3/30: 100%|██████████| 8/8 [00:13<00:00,  1.68s/it, loss=0.0531]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0603)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 4/30: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it, loss=0.0382]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0429)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 5/30: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it, loss=0.0318]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0337)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 6/30: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it, loss=0.0284]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0285)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 7/30: 100%|██████████| 8/8 [00:13<00:00,  1.65s/it, loss=0.0225]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0248)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 8/30: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0226)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 9/30: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, loss=0.0197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 10/30: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it, loss=0.0189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0196)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 11/30: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it, loss=0.0187]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0186)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 12/30: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it, loss=0.0178]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0176)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 13/30: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, loss=0.0172]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0170)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 14/30: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it, loss=0.0163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0164)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 15/30: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, loss=0.0153]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0158)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 16/30: 100%|██████████| 8/8 [00:07<00:00,  1.01it/s, loss=0.0185]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0157)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 17/30: 100%|██████████| 8/8 [00:08<00:00,  1.02s/it, loss=0.0154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0151)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 18/30: 100%|██████████| 8/8 [00:08<00:00,  1.09s/it, loss=0.0154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0148)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 19/30: 100%|██████████| 8/8 [00:07<00:00,  1.03it/s, loss=0.0141]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0144)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 20/30: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it, loss=0.0142]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0143)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 21/30: 100%|██████████| 8/8 [00:07<00:00,  1.06it/s, loss=0.0145]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0142)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 22/30: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it, loss=0.015]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0140)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 23/30: 100%|██████████| 8/8 [00:07<00:00,  1.04it/s, loss=0.0138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0138)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 24/30: 100%|██████████| 8/8 [00:08<00:00,  1.01s/it, loss=0.0151]\n",
            "[anime] Época 25/30: 100%|██████████| 8/8 [00:12<00:00,  1.56s/it, loss=0.0148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0137)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 26/30: 100%|██████████| 8/8 [00:07<00:00,  1.05it/s, loss=0.0154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0136)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 27/30: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, loss=0.0161]\n",
            "[anime] Época 28/30: 100%|██████████| 8/8 [00:12<00:00,  1.57s/it, loss=0.0124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_anime_best.pt (loss=0.0134)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Época 29/30: 100%|██████████| 8/8 [00:07<00:00,  1.04it/s, loss=0.0136]\n",
            "[anime] Época 30/30: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s, loss=0.0135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Entrenamiento completo de anime. Mejor loss: 0.0134\n",
            "\n",
            "🚀 Entrenando modelo para: PAINTING (30 épocas)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 1/30: 100%|██████████| 6/6 [09:15<00:00, 92.55s/it, loss=0.0869]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.1103)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 2/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0628)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 3/30: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it, loss=0.0436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0441)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 4/30: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it, loss=0.0399]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0399)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 5/30: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it, loss=0.0355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0376)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 6/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0345]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0344)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 7/30: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it, loss=0.0306]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 8/30: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it, loss=0.0284]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0291)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 9/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0273)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 10/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0261)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 11/30: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it, loss=0.0246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0248)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 12/30: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it, loss=0.0254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0237)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 13/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0229)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 14/30: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it, loss=0.0217]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0222)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 15/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0217)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 16/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0216]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0211)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 17/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0211]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0207)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 18/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0202)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 19/30: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it, loss=0.0211]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 20/30: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it, loss=0.0195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0197)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 21/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0192]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0197)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 22/30: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it, loss=0.0197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0194)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 23/30: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it, loss=0.0177]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0192)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 24/30: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it, loss=0.0208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0191)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 25/30: 100%|██████████| 6/6 [00:06<00:00,  1.17s/it, loss=0.0206]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0190)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 26/30: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it, loss=0.019]\n",
            "[painting] Época 27/30: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it, loss=0.0188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0189)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 28/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.0189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Nuevo mejor modelo guardado en /content/drive/My Drive/StyleMatcher/model_painting_best.pt (loss=0.0188)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Época 29/30: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it, loss=0.019]\n",
            "[painting] Época 30/30: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it, loss=0.0184]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Entrenamiento completo de painting. Mejor loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformaciones\n",
        "def image_loader(img, max_size=400, shape=None):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert(\"RGB\")\n",
        "    if max(img.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(img.size)\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])])\n",
        "    image = transform(img).unsqueeze(0)\n",
        "    return image.to(device)\n",
        "\n",
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().squeeze(0)\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return image\n",
        "\n",
        "# Modelo VGG19\n",
        "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Extracción de características\n",
        "def get_features(image, model, layers=None):\n",
        "    if layers is None:\n",
        "        layers = {\n",
        "            '0': 'conv1_1',\n",
        "            '5': 'conv2_1',\n",
        "            '10': 'conv3_1',\n",
        "            '19': 'conv4_1',\n",
        "            '21': 'conv4_2',\n",
        "            '28': 'conv5_1'\n",
        "        }\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    _, d, h, w = tensor.size()\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    return torch.mm(tensor, tensor.t())\n",
        "\n",
        "# Función principal de inferencia\n",
        "def style_transfer(content_img, style_img):\n",
        "    content = image_loader(content_img)\n",
        "    style = image_loader(style_img, shape=content.shape[-2:])\n",
        "\n",
        "    content_features = get_features(content, vgg)\n",
        "    style_features = get_features(style, vgg)\n",
        "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    target = content.clone().requires_grad_(True).to(device)\n",
        "    optimizer = optim.Adam([target], lr=0.003)\n",
        "\n",
        "    style_weights = {\n",
        "        'conv1_1': 1.0,\n",
        "        'conv2_1': 0.75,\n",
        "        'conv3_1': 0.2,\n",
        "        'conv4_1': 0.2,\n",
        "        'conv5_1': 0.2\n",
        "    }\n",
        "\n",
        "    content_weight = 1e4\n",
        "    style_weight = 1e2\n",
        "\n",
        "    steps = 200\n",
        "    for i in range(steps):\n",
        "        target_features = get_features(target, vgg)\n",
        "        content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
        "\n",
        "        style_loss = 0\n",
        "        for layer in style_weights:\n",
        "            target_feature = target_features[layer]\n",
        "            target_gram = gram_matrix(target_feature)\n",
        "            style_gram = style_grams[layer]\n",
        "            layer_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "            style_loss += layer_loss / (target_feature.shape[1] ** 2)\n",
        "\n",
        "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return im_convert(target)\n",
        "\n",
        "# Interfaz Gradio\n",
        "gr.Interface(\n",
        "    fn=style_transfer,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Imagen base (contenido)\", type=\"pil\"),\n",
        "        gr.Image(label=\"Imagen estilo\", type=\"pil\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Resultado estilizado\"),\n",
        "    title=\"🎨 Style Transfer con VGG19\",\n",
        "    description=\"Generación de imagen estilizada combinando contenido y estilo. No necesita entrenamiento.\",\n",
        ").launch(debug=True)\n"
      ],
      "metadata": {
        "id": "_JvfAOxQV_x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = anime_dataset[0]\n",
        "print(\"Image:\", sample[\"image\"].shape)\n",
        "print(\"Pose:\", sample[\"pose\"].shape)\n",
        "print(\"Identity:\", sample[\"identity\"].shape)\n",
        "print(\"Style:\", sample[\"style\"].shape)"
      ],
      "metadata": {
        "id": "UIeIu1uT8iBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(DataLoader(StylizerDatasetV3(\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\"\n",
        "), batch_size=1)))\n",
        "\n",
        "print(\"🟢 image:\", sample[\"image\"].shape)\n",
        "print(\"🟠 pose:\", sample[\"pose\"].shape)\n",
        "print(\"🔵 identity:\", sample[\"identity\"].shape)\n",
        "print(\"🔴 style:\", sample[\"style\"].shape)"
      ],
      "metadata": {
        "id": "ioRt0qh8OXWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener 1 batch del dataloader\n",
        "dataset = StylizerDatasetV3(\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\"\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "model = StylizerUNet(id_dim=512, style_dim=1472).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ejecutar paso forward y mostrar formas\n",
        "batch = next(iter(loader))\n",
        "img = batch[\"image\"]\n",
        "pose = batch[\"pose\"]\n",
        "id_vec = batch[\"identity\"]\n",
        "style_vec = batch[\"style\"]\n",
        "\n",
        "print(\"📦 image:\", img.shape)\n",
        "print(\"🟠 pose:\", pose.shape)\n",
        "print(\"🔵 identity:\", id_vec.shape)\n",
        "print(\"🔴 style:\", style_vec.shape)\n",
        "\n",
        "# Mover al dispositivo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pose = pose.to(device)\n",
        "id_vec = id_vec.to(device)\n",
        "style_vec = style_vec.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(pose, id_vec, style_vec)\n",
        "    print(\"✅ output:\", output.shape)\n"
      ],
      "metadata": {
        "id": "gaZ8UNjPPfKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🎯 output.shape:\", outputs.shape)\n",
        "print(\"📦 image.shape:\", img.shape)\n",
        "print(\"✅ same shape:\", outputs.shape == img.shape)\n",
        "print(\"📊 output dtype:\", outputs.dtype)\n",
        "print(\"📊 image dtype:\", img.dtype)\n",
        "print(\"🧠 output device:\", outputs.device)\n",
        "print(\"🧠 image device:\", img.device)\n"
      ],
      "metadata": {
        "id": "4jhQHecjQV9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    dataset_name=\"anime\",\n",
        "    image_dir=\"/content/drive/My Drive/StyleMatcher/dataset/train/anime\",\n",
        "    style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    input_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    save_path=\"/content/drive/My Drive/StyleMatcher/model_anime_best.pt\",\n",
        "    checkpoint_path=None,\n",
        "    epochs=30\n",
        ")\n"
      ],
      "metadata": {
        "id": "_02r2jcV65ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL6BA1_8EqWN"
      },
      "source": [
        "#Ejemplo con 1 imagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWzChjWJo6Um"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset # Ensure Dataset is imported\n",
        "from torchvision import transforms     # Ensure transforms is imported\n",
        "from PIL import Image                # Ensure Image is imported\n",
        "import numpy as np                   # Ensure numpy is imported\n",
        "import json                          # Ensure json is imported\n",
        "import cv2                           # Ensure cv2 is imported\n",
        "\n",
        "# (StylizerDataset definition remains the same as in the previous suggestion)\n",
        "class StylizerDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir, name_prefix='test_01'):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.name = name_prefix\n",
        "        self.img_size = (256, 256)\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.img_size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def load_pose_heatmap(self, path_json):\n",
        "        with open(path_json, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        heatmap = np.zeros(self.img_size, dtype=np.uint8)\n",
        "        for lm in data.get('pose', []):\n",
        "            x = int(lm['x'] * self.img_size[1])\n",
        "            y = int(lm['y'] * self.img_size[0])\n",
        "            cv2.circle(heatmap, (x, y), 4, 255, -1)\n",
        "        return torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(f\"{self.input_dir}/{self.name}.jpg\").convert('RGB')\n",
        "        pose = self.load_pose_heatmap(f\"{self.input_dir}/{self.name}_pose.json\")\n",
        "\n",
        "        # Load identity and ensure it's a 1D vector (excluding batch dim)\n",
        "        identity = np.load(f\"{self.input_dir}/{self.name}_identity.npy\").squeeze() # Remove potential extra dimensions\n",
        "        identity = torch.tensor(identity, dtype=torch.float32)\n",
        "        if identity.ndim == 0: # Handle case where squeeze results in scalar\n",
        "             identity = identity.unsqueeze(0) # Make it 1D: [512] or [64] etc.\n",
        "\n",
        "        # Load style and ensure it's a 1D vector (excluding batch dim)\n",
        "        style = np.load(f\"{self.input_dir}/{self.name}_style.npy\").squeeze() # Remove potential extra dimensions\n",
        "        style = torch.tensor(style, dtype=torch.float32)\n",
        "        if style.ndim == 0: # Handle case where squeeze results in scalar\n",
        "            style = style.unsqueeze(0) # Make it 1D: [64] or [512] etc.\n",
        "\n",
        "        target = Image.open(f\"{self.target_dir}/{self.name}_target.jpg\").convert('RGB')\n",
        "\n",
        "        return {\n",
        "            'image': self.transform(img),\n",
        "            'pose': pose,\n",
        "            'identity': identity,\n",
        "            'style': style,\n",
        "            'target': self.transform(target)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "# Preparar dataset\n",
        "input_dir = \"/content/drive/My Drive/StyleMatcher/dataset_mini/inputs\"\n",
        "target_dir = \"/content/drive/My Drive/StyleMatcher/dataset_mini/targets\"\n",
        "# Ensure the dataset file exists for 'test_01' with identity and style embeddings\n",
        "try:\n",
        "    dataset = StylizerDataset(input_dir, target_dir)\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Make sure the dataset files exist at {input_dir} and {target_dir}. Details: {e}\")\n",
        "    # You might want to exit or handle this case appropriately\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Inicializar modelo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MiniStylizedUNet(style_dim=64, identity_dim=512).to(device)\n",
        "\n",
        "# Pérdida y optimizador\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Función para mostrar resultados\n",
        "def show_result(pred, target, epoch):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "    # Convert tensor to numpy and change shape from [C, H, W] to [H, W, C] for matplotlib\n",
        "    ax[0].imshow(pred.permute(1, 2, 0).detach().cpu().numpy())\n",
        "    ax[0].set_title(\"Generado\")\n",
        "    ax[1].imshow(target.permute(1, 2, 0).detach().cpu().numpy())\n",
        "    ax[1].set_title(\"Esperado\")\n",
        "    for a in ax:\n",
        "        a.axis('off')\n",
        "    plt.suptitle(f\"Resultado Epoch {epoch}\")\n",
        "    plt.show()\n",
        "\n",
        "# Entrenamiento\n",
        "model.train()\n",
        "for epoch in range(300):\n",
        "    for batch in dataloader:\n",
        "        img = batch['image'].to(device)\n",
        "        pose = batch['pose'].to(device)\n",
        "        identity = batch['identity'].to(device)\n",
        "        style = batch['style'].to(device)\n",
        "        target = batch['target'].to(device)  # <-- Línea añadida\n",
        "        strength = torch.ones((identity.size(0), 1), device=device)\n",
        "\n",
        "        output = model(img, pose, style, identity, strength)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == 299:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "        show_result(output[0], target[0], epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjX_kmVM-G4R"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Script para verificar resolución y diversidad de pose en imágenes\n",
        "# ----------------------------------------------\n",
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Rutas\n",
        "dataset_dir = \"/content/drive/My Drive/StyleMatcher/dataset/train\"\n",
        "categories = [\"anime\", \"painting\"] # Asegúrate de que estos nombres coincidan exactamente con los nombres de las carpetas\n",
        "\n",
        "# --- Añadir verificación de directorios ---\n",
        "print(\"Verificando la existencia de las carpetas de categorías:\")\n",
        "for category in categories:\n",
        "    folder_path = os.path.join(dataset_dir, category)\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"❌ ERROR: La carpeta '{folder_path}' no existe.\")\n",
        "        # Considerar salir o manejar este error de forma diferente\n",
        "        # exit() # Descomentar para detener la ejecución si una carpeta no existe\n",
        "    else:\n",
        "        print(f\"✅ La carpeta '{folder_path}' existe.\")\n",
        "print(\"-\" * 30)\n",
        "# -----------------------------------------\n",
        "\n",
        "\n",
        "# Inicializar MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose_detector = mp_pose.Pose(static_image_mode=True)\n",
        "\n",
        "# Configuración\n",
        "min_resolution = (200, 200)  # resolución mínima aceptable\n",
        "max_images_per_category = 20\n",
        "\n",
        "# Visualización\n",
        "def show_pose_landmarks(image_path, landmarks, category):\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    for lm in landmarks:\n",
        "        cx, cy = int(lm.x * w), int(lm.y * h)\n",
        "        cv2.circle(img_rgb, (cx, cy), 3, (0, 255, 0), -1)\n",
        "\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(f\"{category} - Pose detectada\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Proceso\n",
        "for category in categories:\n",
        "    folder = os.path.join(dataset_dir, category)\n",
        "    # Ya verificamos si la carpeta existe, ahora podemos listar sin riesgo de FileNotFoundError si la verificación no detuvo la ejecución.\n",
        "    images = [f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    print(f\"\\n🔍 Verificando categoría: {category} (máx. {max_images_per_category})\")\n",
        "\n",
        "    count = 0\n",
        "    # Limitar el número de imágenes a procesar para evitar mostrar demasiadas figuras si hay muchas imágenes.\n",
        "    for img_file in tqdm(images[:max_images_per_category], desc=f\"Procesando imágenes de {category}\"):\n",
        "        img_path = os.path.join(folder, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"⚠️ No se pudo leer la imagen: {img_file}\")\n",
        "            continue\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "        if h < min_resolution[0] or w < min_resolution[1]:\n",
        "            print(f\"⚠️ Imagen muy pequeña: {img_file} ({w}x{h})\")\n",
        "            continue\n",
        "\n",
        "        # Pose detection\n",
        "        results = pose_detector.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        if not results or not results.pose_landmarks:\n",
        "            print(f\"❌ Sin pose detectada: {img_file}\")\n",
        "            continue\n",
        "\n",
        "        # Mostrar imagen con pose\n",
        "        show_pose_landmarks(img_path, results.pose_landmarks.landmark, category)\n",
        "        count += 1\n",
        "\n",
        "        if count >= max_images_per_category:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zet4DV3dYeuB"
      },
      "source": [
        "# Script para generar dataset_final con 900 imágenes de anime y pintura (train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R90TE9vtYgDI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import torchvision.transforms as transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision.models import vgg19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Ngqqg1YlxM"
      },
      "source": [
        "# Configuración de rutas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGNo3DdCYo1F"
      },
      "outputs": [],
      "source": [
        "source_dir = \"/content/drive/My Drive/StyleMatcher/dataset/train\"\n",
        "base_input_out = \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs\"\n",
        "base_target_out = \"/content/drive/My Drive/StyleMatcher/dataset_final/targets\"\n",
        "\n",
        "input_dirs = {\n",
        "    \"anime\": os.path.join(base_input_out, \"anime_inputs\"),\n",
        "    \"painting\": os.path.join(base_input_out, \"painting_inputs\")\n",
        "}\n",
        "target_dirs = {\n",
        "    \"anime\": os.path.join(base_target_out, \"anime\"),\n",
        "    \"painting\": os.path.join(base_target_out, \"painting\")\n",
        "}\n",
        "\n",
        "for d in input_dirs.values(): os.makedirs(d, exist_ok=True)\n",
        "for d in target_dirs.values(): os.makedirs(d, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beIXTpwQYrrj"
      },
      "source": [
        "# Modelos necesarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPTRvJtNYuQw"
      },
      "outputs": [],
      "source": [
        "vgg = vgg19(pretrained=True).features.eval()\n",
        "vgg_layers = [0, 5]  # capas 0 y 5 para estilo más informativo\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "transform_style = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_identity = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_target = transforms.Compose([\n",
        "    transforms.Resize((256, 256))\n",
        "])\n",
        "\n",
        "mp_pose = mp.solutions.pose.Pose(static_image_mode=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKwtEToAY24h"
      },
      "source": [
        "# Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbQGIoSeY3of"
      },
      "outputs": [],
      "source": [
        "def extract_pose_landmarks(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = mp_pose.process(image_rgb)\n",
        "    if not results.pose_landmarks:\n",
        "        return None\n",
        "    keypoints = [{\"x\": lm.x, \"y\": lm.y} for lm in results.pose_landmarks.landmark]\n",
        "    return keypoints\n",
        "\n",
        "def extract_face_embedding(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    tensor = transform_identity(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        return facenet(tensor).numpy()\n",
        "\n",
        "def extract_style_vector(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    tensor = transform_style(image).unsqueeze(0)\n",
        "    features = []\n",
        "    x = tensor\n",
        "    for i, layer in enumerate(vgg):\n",
        "        x = layer(x)\n",
        "        if i in vgg_layers:\n",
        "            pooled = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "            features.append(pooled)\n",
        "    final_style = torch.cat(features, dim=1).numpy()\n",
        "    return final_style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usNGeSrJY81l"
      },
      "source": [
        "# Proceso principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWI9qmXRY-jy"
      },
      "outputs": [],
      "source": [
        "categories = [\"anime\", \"painting\"]\n",
        "max_images = 900\n",
        "\n",
        "for category in categories:\n",
        "    folder = os.path.join(source_dir, category)\n",
        "    images = sorted([f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])[:max_images]\n",
        "\n",
        "    for idx, filename in tqdm(enumerate(images), total=len(images), desc=f\"Procesando {category}\"):\n",
        "        path = os.path.join(folder, filename)\n",
        "        base_name = f\"{category}_{str(idx).zfill(4)}\"\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img.save(os.path.join(input_dirs[category], f\"{base_name}.jpg\"))\n",
        "            transform_target(img).save(os.path.join(target_dirs[category], f\"{base_name}_target.jpg\"))\n",
        "\n",
        "            pose = extract_pose_landmarks(path)\n",
        "            if pose is None: continue\n",
        "            with open(os.path.join(input_dirs[category], f\"{base_name}_pose.json\"), 'w') as f:\n",
        "                json.dump({\"pose\": pose}, f)\n",
        "\n",
        "            identity = extract_face_embedding(path)\n",
        "            np.save(os.path.join(input_dirs[category], f\"{base_name}_identity.npy\"), identity)\n",
        "\n",
        "            style = extract_style_vector(path)\n",
        "            np.save(os.path.join(input_dirs[category], f\"{base_name}_style.npy\"), style)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXnzawa6tiJO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import vgg19\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "# Modelos\n",
        "vgg = vgg19(pretrained=True).features.eval()\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "mp_pose = mp.solutions.pose.Pose(static_image_mode=True)\n",
        "\n",
        "# Transformaciones\n",
        "transform_style = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_identity = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Funciones auxiliares\n",
        "def extract_pose_landmarks(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return None\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = mp_pose.process(image_rgb)\n",
        "    if not results.pose_landmarks:\n",
        "        return None\n",
        "    return [{\"x\": lm.x, \"y\": lm.y} for lm in results.pose_landmarks.landmark]\n",
        "\n",
        "def extract_face_embedding(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform_identity(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        return facenet(img_tensor).numpy()\n",
        "\n",
        "def extract_style_vector(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform_style(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        x = img_tensor\n",
        "        for i, layer in enumerate(vgg):\n",
        "            x = layer(x)\n",
        "            if i == 0:  # conv1_1\n",
        "                break\n",
        "        pooled = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "    return pooled.numpy()\n",
        "\n",
        "# Carpetas\n",
        "bases = {\n",
        "    \"anime\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    \"painting\": \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_inputs\"\n",
        "}\n",
        "\n",
        "for categoria, base_dir in bases.items():\n",
        "    print(f\"🧩 Procesando {categoria}\")\n",
        "    files = sorted([f for f in os.listdir(base_dir) if f.endswith(\".jpg\")])\n",
        "\n",
        "    for file in tqdm(files):\n",
        "        path_base = os.path.join(base_dir, file)\n",
        "        name = os.path.splitext(file)[0]\n",
        "\n",
        "        # Pose\n",
        "        pose_path = os.path.join(base_dir, name + \"_pose.json\")\n",
        "        if not os.path.exists(pose_path):\n",
        "            pose = extract_pose_landmarks(path_base)\n",
        "            if pose is not None:\n",
        "                with open(pose_path, 'w') as f:\n",
        "                    json.dump({\"pose\": pose}, f)\n",
        "\n",
        "        # Identidad\n",
        "        identity_path = os.path.join(base_dir, name + \"_identity.npy\")\n",
        "        if not os.path.exists(identity_path):\n",
        "            try:\n",
        "                identity = extract_face_embedding(path_base)\n",
        "                np.save(identity_path, identity)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Estilo\n",
        "        style_path = os.path.join(base_dir.replace(\"inputs\", \"styles\"), name + \"_style.npy\")\n",
        "        if not os.path.exists(style_path):\n",
        "            try:\n",
        "                style = extract_style_vector(path_base)\n",
        "                os.makedirs(os.path.dirname(style_path), exist_ok=True)\n",
        "                np.save(style_path, style)\n",
        "            except:\n",
        "                continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generador"
      ],
      "metadata": {
        "id": "97G0d7njBIO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import torchvision.transforms as transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "# 🔧 Rutas base\n",
        "ruta_base = \"/content/drive/My Drive/StyleMatcher/dataset_final/inputs\"\n",
        "ruta_target = \"/content/drive/My Drive/StyleMatcher/dataset_final/targets\"\n",
        "categorias = {\n",
        "    \"anime\": os.path.join(ruta_base, \"anime_styles\"),\n",
        "    \"painting\": os.path.join(ruta_base, \"painting_styles\")\n",
        "}\n",
        "\n",
        "# 🔧 Transformaciones\n",
        "vgg = vgg19(pretrained=True).features.eval()\n",
        "vgg_layers = [0]\n",
        "\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "transform_style = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_identity = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_target = transforms.Compose([\n",
        "    transforms.Resize((256, 256))\n",
        "])\n",
        "\n",
        "mp_pose = mp.solutions.pose.Pose(static_image_mode=True)\n",
        "\n",
        "# 🔧 Funciones\n",
        "def extract_pose_landmarks(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return None\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = mp_pose.process(image_rgb)\n",
        "    if not results.pose_landmarks:\n",
        "        return None\n",
        "    return [{\"x\": lm.x, \"y\": lm.y} for lm in results.pose_landmarks.landmark]\n",
        "\n",
        "def extract_face_embedding(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    tensor = transform_identity(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        emb = facenet(tensor)\n",
        "    return emb.numpy()\n",
        "\n",
        "def extract_style_vector(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    tensor = transform_style(image).unsqueeze(0)\n",
        "    x = tensor\n",
        "    for i, layer in enumerate(vgg):\n",
        "        x = layer(x)\n",
        "        if i == vgg_layers[-1]:\n",
        "            break\n",
        "    pooled = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "    return pooled.numpy()\n",
        "\n",
        "# 🔁 Recorremos cada categoría\n",
        "for clase, carpeta in categorias.items():\n",
        "    output_input_dir = os.path.join(ruta_base, f\"{clase}_inputs\")\n",
        "    output_target_dir = os.path.join(ruta_target, clase)\n",
        "    os.makedirs(output_input_dir, exist_ok=True)\n",
        "    os.makedirs(output_target_dir, exist_ok=True)\n",
        "\n",
        "    imagenes = sorted([f for f in os.listdir(carpeta) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "    for idx, nombre_archivo in tqdm(enumerate(imagenes), total=len(imagenes), desc=f\"Procesando {clase}\"):\n",
        "        nombre_base = f\"{clase}_{str(idx).zfill(4)}\"\n",
        "        ruta_imagen = os.path.join(carpeta, nombre_archivo)\n",
        "\n",
        "        try:\n",
        "            # Target\n",
        "            img = Image.open(ruta_imagen).convert(\"RGB\")\n",
        "            target = transform_target(img)\n",
        "            target.save(os.path.join(output_target_dir, f\"{nombre_base}_target.jpg\"))\n",
        "\n",
        "            # Pose\n",
        "            pose = extract_pose_landmarks(ruta_imagen)\n",
        "            if pose is None:\n",
        "                continue\n",
        "            with open(os.path.join(output_input_dir, f\"{nombre_base}_pose.json\"), \"w\") as f:\n",
        "                json.dump({\"pose\": pose}, f)\n",
        "\n",
        "            # Identidad\n",
        "            identidad = extract_face_embedding(ruta_imagen)\n",
        "            np.save(os.path.join(output_input_dir, f\"{nombre_base}_identity.npy\"), identidad)\n",
        "\n",
        "            # Estilo\n",
        "            estilo = extract_style_vector(ruta_imagen)\n",
        "            np.save(os.path.join(output_input_dir, f\"{nombre_base}_style.npy\"), estilo)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Fallo con {nombre_archivo}: {e}\")\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "EOxXVyzgHM-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM7wVcTmYfX4"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbir5l9qm9wn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Configuración general\n",
        "# -------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"🖥️ Usando dispositivo:\", device)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "INPUT_VECTOR_SIZE = 1025  # 512 pose + 449 identidad + 64 estilo\n",
        "EPOCHS = 40\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Dataset personalizado\n",
        "# -------------------------------------------------\n",
        "class StylizerDatasetV2(Dataset):\n",
        "    def __init__(self, input_dir, target_dir, style_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.style_dir = style_dir\n",
        "        self.names = []\n",
        "\n",
        "        # Filtrar solo los ejemplos que tienen todos los archivos\n",
        "        for file in os.listdir(input_dir):\n",
        "            if not file.endswith(\".jpg\"):\n",
        "                continue\n",
        "            name = file.split(\".\")[0]\n",
        "            pose_path = os.path.join(input_dir, name + \"_pose.json\")\n",
        "            identity_path = os.path.join(input_dir, name + \"_identity.npy\")\n",
        "            style_path = os.path.join(style_dir, name + \"_style.npy\")\n",
        "            target_path = os.path.join(target_dir, name + \"_target.jpg\")\n",
        "\n",
        "            if all(os.path.exists(p) for p in [pose_path, identity_path, style_path, target_path]):\n",
        "                self.names.append(name)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "\n",
        "        # Cargar imagen target\n",
        "        target = Image.open(os.path.join(self.target_dir, name + \"_target.jpg\")).convert(\"RGB\")\n",
        "        target = target.resize((256, 256))\n",
        "        target = torch.FloatTensor(np.array(target)).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        # Cargar pose\n",
        "        with open(os.path.join(self.input_dir, name + \"_pose.json\"), \"r\") as f:\n",
        "            pose_data = json.load(f)[\"pose\"]\n",
        "        pose_map = np.zeros((512,), dtype=np.float32)\n",
        "        for i, lm in enumerate(pose_data[:256]):\n",
        "            pose_map[i * 2] = lm[\"x\"]\n",
        "            pose_map[i * 2 + 1] = lm[\"y\"]\n",
        "\n",
        "        # Cargar identidad\n",
        "        identity = np.load(os.path.join(self.input_dir, name + \"_identity.npy\")).flatten()\n",
        "        identity = identity[:449]  # Limitamos a 449 si hace falta\n",
        "\n",
        "        # Cargar estilo\n",
        "        style = np.load(os.path.join(self.style_dir, name + \"_style.npy\")).flatten()\n",
        "\n",
        "        # Concatenar\n",
        "        vector = np.concatenate([pose_map, identity, style], axis=0)\n",
        "        vector = torch.tensor(vector, dtype=torch.float32)\n",
        "\n",
        "        return vector, target\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Modelo U-Net modificado\n",
        "# -------------------------------------------------\n",
        "class MiniStylizedUNet(nn.Module):\n",
        "    def __init__(self, input_vector_size):\n",
        "        super(MiniStylizedUNet, self).__init__()\n",
        "        self.input_vector_size = input_vector_size\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_vector_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256 * 4 * 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # 64x64\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=4, padding=0),     # 256x256\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Entrenamiento con Fine-Tuning\n",
        "# -------------------------------------------------\n",
        "def entrenar(dataset_dir, style_dir, target_dir, nombre_categoria):\n",
        "    dataset = StylizerDatasetV2(dataset_dir, target_dir, style_dir)\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"⚠️ No se encontraron ejemplos completos para {nombre_categoria}.\")\n",
        "        return\n",
        "\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1)\n",
        "\n",
        "    model = MiniStylizedUNet(INPUT_VECTOR_SIZE).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    loss_fn = nn.L1Loss()\n",
        "\n",
        "    model_path = f\"/content/drive/My Drive/StyleMatcher/model_{nombre_categoria}_best.pt\"\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    # 🔁 Intentar cargar modelo preentrenado\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"📦 Cargando modelo existente: {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(\"✅ Modelo cargado correctamente\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\n📘 Época {epoch+1}/{EPOCHS} — Categoría: {nombre_categoria.upper()}\")\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for vec, target in tqdm(train_loader, desc=\"Entrenando\"):\n",
        "            vec = vec.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(vec)\n",
        "            loss = loss_fn(output, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"📉 Pérdida entrenamiento: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # 🔍 Validación\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for vec, target in val_loader:\n",
        "                vec = vec.to(device)\n",
        "                target = target.to(device)\n",
        "                output = model(vec)\n",
        "                val_loss += loss_fn(output, target).item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"✅ Validación pérdida: {avg_val_loss:.4f}\")\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print(f\"💾 Nuevo mejor modelo guardado en {model_path}\")\n",
        "\n",
        "    print(f\"🏁 Fine-tuning de {nombre_categoria} completado\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Lanzar entrenamientos (Fine-Tuning)\n",
        "# -------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Comenzando fine-tuning...\")\n",
        "\n",
        "    entrenar(\n",
        "        dataset_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "        style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "        target_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/targets/anime\",\n",
        "        nombre_categoria=\"anime\"\n",
        "    )\n",
        "\n",
        "    entrenar(\n",
        "        dataset_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_inputs\",\n",
        "        style_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_styles\",\n",
        "        target_dir=\"/content/drive/My Drive/StyleMatcher/dataset_final/targets/painting\",\n",
        "        nombre_categoria=\"painting\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4urhuh7qTtJ"
      },
      "outputs": [],
      "source": [
        "#ruta imagenes\n",
        "/content/drive/My Drive/StyleMatcher/dataset/train/anime\n",
        "/content/drive/My Drive/StyleMatcher/dataset/train/painting\n",
        "#ruta estilos\n",
        "/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_styles\n",
        "/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_styles\n",
        "#ruta posiciones e identidad\n",
        "/content/drive/My Drive/StyleMatcher/dataset_final/inputs/painting_inputs\n",
        "/content/drive/My Drive/StyleMatcher/dataset_final/inputs/anime_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MEScAgm_IH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781,
          "referenced_widgets": [
            "8b18a9a95cd44a9db7b2e1e6a128042c",
            "7de7b6f8ceb04d258f03e195973a20f5",
            "d0e4504ed9c34a61952a4837ce2e42cc",
            "f7deac7aaca04bf58dc3b2c5ed601b87",
            "9f7413c9e7fd4344ba7165a15f2b890e",
            "a46e6998970d4c7287b0e94c284b8039",
            "1ce235bba58848339f3ec892c7d957f4",
            "4cbe4d1618824efe8dc96028c7ede3bb",
            "bc0a190c0e994a8e9b04cc2dc5934e62",
            "80454c34573c4b84a09529afada8e34d",
            "68979af49f2a476ea28bbd496156b28b"
          ]
        },
        "outputId": "37665a2c-2002-448a-d801-22db3b275e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b18a9a95cd44a9db7b2e1e6a128042c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9b4bd9954ee632a5bf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b4bd9954ee632a5bf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9b4bd9954ee632a5bf.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# === INTERFAZ GRADIO FINAL ===\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Modelo de Estilo: VGG19 ---\n",
        "vgg = models.vgg19(pretrained=True).features[:29].eval().to(device)\n",
        "for p in vgg.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "def extract_style(img_tensor):\n",
        "    layers = [0, 5, 10, 19, 28]\n",
        "    features = []\n",
        "    x = img_tensor\n",
        "    for i, layer in enumerate(vgg):\n",
        "        x = layer(x)\n",
        "        if i in layers:\n",
        "            features.append(x.mean([2, 3]))\n",
        "    return torch.cat(features, dim=1)\n",
        "\n",
        "# --- Modelo de Identidad: FaceNet ---\n",
        "!pip install facenet-pytorch --quiet\n",
        "\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# --- Modelo de Estilización ---\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        if use_dropout:\n",
        "            self.block.add_module(\"dropout\", nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class StylizerUNet(nn.Module):\n",
        "    def __init__(self, id_dim=512, style_dim=1472):\n",
        "        super().__init__()\n",
        "        self.emb_dim = id_dim + style_dim\n",
        "        self.embedding_expand = nn.Sequential(\n",
        "            nn.Linear(self.emb_dim, 512 * 4 * 4),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.enc1 = UNetBlock(1, 64, down=True)\n",
        "        self.enc2 = UNetBlock(64, 128, down=True)\n",
        "        self.enc3 = UNetBlock(128, 256, down=True)\n",
        "        self.enc4 = UNetBlock(256, 512, down=True)\n",
        "        self.enc5 = UNetBlock(512, 512, down=True)\n",
        "        self.enc6 = UNetBlock(512, 512, down=True)\n",
        "        self.enc7 = UNetBlock(512, 512, down=True)\n",
        "        self.middle = nn.Sequential(nn.Conv2d(1024, 512, 3, padding=1), nn.ReLU(True))\n",
        "        self.dec1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
        "        self.dec2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.dec3 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.dec4 = UNetBlock(1024, 512, down=False)\n",
        "        self.dec5 = UNetBlock(768, 256, down=False)\n",
        "        self.dec6 = UNetBlock(384, 128, down=False)\n",
        "        self.dec7 = UNetBlock(192, 64, down=False)\n",
        "        self.final = nn.Conv2d(65, 3, 3, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, pose_map, identity_vec, style_vec):\n",
        "        emb = torch.cat([identity_vec, style_vec], dim=1)\n",
        "        emb = self.embedding_expand(emb).view(-1, 512, 4, 4)\n",
        "        e1 = self.enc1(pose_map)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "        e6 = self.enc6(e5)\n",
        "        e7 = self.enc7(e6)\n",
        "        emb_up = F.interpolate(emb, size=e7.shape[2:])\n",
        "        bottleneck = self.middle(torch.cat([e7, emb_up], dim=1))\n",
        "        d1 = self.dec1(bottleneck)\n",
        "        d2 = self.dec2(torch.cat([d1, e6], dim=1))\n",
        "        d3 = self.dec3(torch.cat([d2, e5], dim=1))\n",
        "        d4 = self.dec4(torch.cat([d3, e4], dim=1))\n",
        "        d5 = self.dec5(torch.cat([d4, e3], dim=1))\n",
        "        d6 = self.dec6(torch.cat([d5, e2], dim=1))\n",
        "        d7 = self.dec7(torch.cat([d6, e1], dim=1))\n",
        "        pose_resized = F.interpolate(pose_map, size=d7.shape[2:])\n",
        "        out = self.final(torch.cat([d7, pose_resized], dim=1))\n",
        "        return self.tanh(out)\n",
        "\n",
        "# --- Transformaciones de entrada ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "def generate_image(pose_img, id_img, style_img, mode):\n",
        "    # Transformaciones\n",
        "    pose = transform(Image.fromarray(pose_img).convert(\"L\")).unsqueeze(0).to(device)\n",
        "    id_tensor = transform(Image.fromarray(id_img).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    style_tensor = transform(Image.fromarray(style_img).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "\n",
        "    # Extraer vectores\n",
        "    identity_vec = facenet(id_tensor)\n",
        "    style_vec = extract_style(style_tensor)\n",
        "\n",
        "    # 🔧 Normalización de vectores (muy importante)\n",
        "    identity_vec = F.normalize(identity_vec, dim=1)\n",
        "    style_vec = F.normalize(style_vec, dim=1)\n",
        "\n",
        "    # Cargar modelo\n",
        "    model = StylizerUNet().to(device)\n",
        "    if mode == \"anime\":\n",
        "        model.load_state_dict(torch.load(\"/content/model_anime_best.pt\", map_location=device))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(\"/content/model_painting_best.pt\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Inferencia\n",
        "    with torch.no_grad():\n",
        "        output = model(pose, identity_vec, style_vec)\n",
        "        output = output.squeeze(0).cpu().permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        return Image.fromarray((output * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "# === INTERFAZ ===\n",
        "gr.Interface(\n",
        "    fn=generate_image,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Mapa de Pose (PNG)\", type=\"numpy\"),\n",
        "        gr.Image(label=\"Imagen de Identidad (PNG)\", type=\"numpy\"),\n",
        "        gr.Image(label=\"Imagen de Estilo (PNG)\", type=\"numpy\"),\n",
        "        gr.Radio([\"anime\", \"painting\"], label=\"Modelo a Usar\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Imagen Estilizada\"),\n",
        "    title=\"StyleMatcher - Generador de Imágenes\",\n",
        "    description=\"Sube una imagen de pose, una de identidad y una de estilo. Elige el modelo (anime o pintura) para generar tu imagen estilizada.\"\n",
        ").launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a2d6d4827c94d09bd4f3dba39701d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27d16ae78c0b40db82b554071767b746",
              "IPY_MODEL_63872ff8e1234b418f17e7777676989b",
              "IPY_MODEL_fda80a3f3ca846228c415c678b13476f"
            ],
            "layout": "IPY_MODEL_d54305ccecab4187952c032b36483762"
          }
        },
        "27d16ae78c0b40db82b554071767b746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb761c9fe04e42aeb85c880a513a4e84",
            "placeholder": "​",
            "style": "IPY_MODEL_7696fb347eb447b5b10c654b02c32b36",
            "value": "100%"
          }
        },
        "63872ff8e1234b418f17e7777676989b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_288a297c30f2478b926f033aefdbb558",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d6baa0739e54cf38320f9ca046d1547",
            "value": 111898327
          }
        },
        "fda80a3f3ca846228c415c678b13476f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5603697fabbb4215b4a4aa86fef9cad3",
            "placeholder": "​",
            "style": "IPY_MODEL_bee803f1778147238c40b59e6d4417b3",
            "value": " 107M/107M [00:00&lt;00:00, 173MB/s]"
          }
        },
        "d54305ccecab4187952c032b36483762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb761c9fe04e42aeb85c880a513a4e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7696fb347eb447b5b10c654b02c32b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "288a297c30f2478b926f033aefdbb558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6baa0739e54cf38320f9ca046d1547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5603697fabbb4215b4a4aa86fef9cad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bee803f1778147238c40b59e6d4417b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b18a9a95cd44a9db7b2e1e6a128042c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7de7b6f8ceb04d258f03e195973a20f5",
              "IPY_MODEL_d0e4504ed9c34a61952a4837ce2e42cc",
              "IPY_MODEL_f7deac7aaca04bf58dc3b2c5ed601b87"
            ],
            "layout": "IPY_MODEL_9f7413c9e7fd4344ba7165a15f2b890e"
          }
        },
        "7de7b6f8ceb04d258f03e195973a20f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46e6998970d4c7287b0e94c284b8039",
            "placeholder": "​",
            "style": "IPY_MODEL_1ce235bba58848339f3ec892c7d957f4",
            "value": "100%"
          }
        },
        "d0e4504ed9c34a61952a4837ce2e42cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cbe4d1618824efe8dc96028c7ede3bb",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc0a190c0e994a8e9b04cc2dc5934e62",
            "value": 111898327
          }
        },
        "f7deac7aaca04bf58dc3b2c5ed601b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80454c34573c4b84a09529afada8e34d",
            "placeholder": "​",
            "style": "IPY_MODEL_68979af49f2a476ea28bbd496156b28b",
            "value": " 107M/107M [00:00&lt;00:00, 416MB/s]"
          }
        },
        "9f7413c9e7fd4344ba7165a15f2b890e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a46e6998970d4c7287b0e94c284b8039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce235bba58848339f3ec892c7d957f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cbe4d1618824efe8dc96028c7ede3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0a190c0e994a8e9b04cc2dc5934e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80454c34573c4b84a09529afada8e34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68979af49f2a476ea28bbd496156b28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}