{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSANT17/ProyectoPDI/blob/main/Entrenamiento_Generador_MultiEstilo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf44966",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cf44966",
        "outputId": "374ed90e-efdd-452b-b344-775eef136ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## 1. Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencias\n"
      ],
      "metadata": {
        "id": "mGmaDhXgVz7a"
      },
      "id": "mGmaDhXgVz7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ce85c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ce85c1",
        "outputId": "5f03c897-b7c9-4533-9419-ce6ff429775a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Collecting gradio\n",
            "  Using cached gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Using cached gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (10.2.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Using cached ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Using cached starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Using cached gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "Using cached gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Using cached ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "## 2. Instalar dependencias necesarias\n",
        "!pip install mediapipe\n",
        "!pip install facenet-pytorch\n",
        "!pip install gradio\n",
        "!pip install torch torchvision\n",
        "!pip install scikit-learn\n",
        "!pip install insightface gradio -q\n",
        "!pip install onnxruntime --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerias\n"
      ],
      "metadata": {
        "id": "jgn6ER9NVLQV"
      },
      "id": "jgn6ER9NVLQV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2c26c8",
      "metadata": {
        "id": "6b2c26c8"
      },
      "outputs": [],
      "source": [
        "## 3. Importar librerías\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from torchvision import models, transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "import mediapipe as mp\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Desarrollo"
      ],
      "metadata": {
        "id": "6vREp1YBVlOz"
      },
      "id": "6vREp1YBVlOz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar categorias\n",
        "# Ruta de entrada y salida\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "root_input = '/content/drive/My Drive/StyleMatcher/dataset'\n",
        "root_output = '/content/drive/My Drive/StyleMatcher/dataset_resized'\n",
        "target_size = (512, 512)\n",
        "\n",
        "def resize_and_save(input_path, output_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    img_paths = glob(os.path.join(input_path, '*'))\n",
        "\n",
        "    for img_path in tqdm(img_paths, desc=f'Redimensionando {input_path}'):\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            resized = cv2.resize(img, target_size)\n",
        "            name = os.path.basename(img_path)\n",
        "            cv2.imwrite(os.path.join(output_path, name), resized)\n",
        "        except Exception as e:\n",
        "            print(f\"Error con {img_path}: {e}\")\n",
        "\n",
        "def process_dataset(root_input, root_output):\n",
        "    for phase in ['train', 'val']:\n",
        "        phase_path = os.path.join(root_input, phase)\n",
        "        for class_name in os.listdir(phase_path):\n",
        "            input_folder = os.path.join(phase_path, class_name)\n",
        "            output_folder = os.path.join(root_output, phase, class_name)\n",
        "            resize_and_save(input_folder, output_folder)\n",
        "\n",
        "# Ejecutamos\n",
        "process_dataset(root_input, root_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "Sz7dE1eWWBOt",
        "outputId": "812d7a92-fefa-4558-9129-7329975e5069"
      },
      "id": "Sz7dE1eWWBOt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Redimensionando /content/drive/My Drive/StyleMatcher/dataset/train/anime:  33%|███▎      | 314/950 [01:38<03:18,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-70c52b58fccb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Ejecutamos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-70c52b58fccb>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(root_input, root_output)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0minput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mresize_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Ejecutamos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-70c52b58fccb>\u001b[0m in \u001b[0;36mresize_and_save\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error con {img_path}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c326976b",
      "metadata": {
        "id": "c326976b"
      },
      "outputs": [],
      "source": [
        "## 4. Dataset personalizado\n",
        "class StyleMatcherDataset(Dataset):\n",
        "    def __init__(self, identity_dir, pose_dir, style_dir, target_dir, transform=None):\n",
        "        self.identity_files = sorted([f for f in os.listdir(identity_dir) if f.endswith('.npy')])\n",
        "        self.pose_files = sorted([f for f in os.listdir(pose_dir) if f.endswith('.png')])\n",
        "        self.style_files = sorted([f for f in os.listdir(style_dir) if f.endswith('.npy')])\n",
        "        self.target_files = sorted([f for f in os.listdir(target_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
        "\n",
        "        self.identity_dir = identity_dir\n",
        "        self.pose_dir = pose_dir\n",
        "        self.style_dir = style_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "            return min(len(self.identity_files), len(self.pose_files), len(self.style_files), len(self.target_files))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        identity_vec = np.load(os.path.join(self.identity_dir, self.identity_files[idx]))\n",
        "        style_vec = np.load(os.path.join(self.style_dir, self.style_files[idx]))\n",
        "\n",
        "        identity_img = np.tile(identity_vec[:3].reshape(1, 1, 3), (256, 256, 1))\n",
        "        style_img = np.tile(style_vec[:3].reshape(1, 1, 3), (256, 256, 1))  # ✅\n",
        "\n",
        "        identity_img = torch.tensor(identity_img, dtype=torch.float32).permute(2, 0, 1)\n",
        "        style_img = torch.tensor(style_img, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "        pose_img = Image.open(os.path.join(self.pose_dir, self.pose_files[idx])).convert(\"RGB\")\n",
        "        target_img = Image.open(os.path.join(self.target_dir, self.target_files[idx])).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            pose_img = self.transform(pose_img)\n",
        "            target_img = self.transform(target_img)\n",
        "\n",
        "        x = torch.cat([identity_img, style_img, pose_img], dim=0)  # [9, 256, 256]\n",
        "        return x, target_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c3249a",
      "metadata": {
        "id": "77c3249a"
      },
      "outputs": [],
      "source": [
        "## 5. Modelo generador (U-Net)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=9, out_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        def block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 4, 2, 1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        self.encoder = nn.Sequential(\n",
        "            block(in_channels, 64),\n",
        "            block(64, 128),\n",
        "            block(128, 256),\n",
        "            block(256, 512)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb52267",
      "metadata": {
        "id": "bbb52267"
      },
      "outputs": [],
      "source": [
        "## 6. Función de entrenamiento\n",
        "def entrenar(model, dataloader, estilo_nombre, epochs=10, lr=1e-4, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.L1Loss()\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in tqdm(dataloader, desc=f\"[{estilo_nombre}] Epoch {epoch+1}/{epochs}\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"🎨 {estilo_nombre} - Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/StyleMatcher/model_{estilo_nombre}_epoch{epoch+1}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Configurar dispositivo y cuDNN para máximo rendimiento\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Usando dispositivo:\", device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ✅ Transformaciones\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ✅ Datasets\n",
        "anime_dataset = StyleMatcherDataset(\n",
        "    identity_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    pose_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/anime_inputs\",\n",
        "    style_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/anime_styles\",\n",
        "    target_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/targets/anime\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "painting_dataset = StyleMatcherDataset(\n",
        "    identity_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/painting_inputs\",\n",
        "    pose_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/painting_inputs\",\n",
        "    style_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/inputs/painting_styles\",\n",
        "    target_dir=\"/content/drive/MyDrive/StyleMatcher/dataset_final/targets/painting\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# ✅ DataLoaders con mejoras\n",
        "anime_loader = DataLoader(anime_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "painting_loader = DataLoader(painting_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ✅ Modelos\n",
        "anime_model = Generator()\n",
        "painting_model = Generator()\n",
        "\n",
        "# ✅ Entrenamiento\n",
        "entrenar(anime_model, anime_loader, estilo_nombre=\"anime\", epochs=10, device=device)\n",
        "entrenar(painting_model, painting_loader, estilo_nombre=\"painting\", epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izfd9E-aBGCA",
        "outputId": "d2270445-0b73-4ebd-a484-0b6ab577d522"
      },
      "id": "izfd9E-aBGCA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 1/10: 100%|██████████| 225/225 [03:04<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 1, Loss: 0.2194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 2/10: 100%|██████████| 225/225 [00:07<00:00, 29.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 2, Loss: 0.1719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 3/10: 100%|██████████| 225/225 [00:07<00:00, 29.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 3, Loss: 0.1671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 4/10: 100%|██████████| 225/225 [00:07<00:00, 29.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 4, Loss: 0.1653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 5/10: 100%|██████████| 225/225 [00:07<00:00, 29.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 5, Loss: 0.1648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 6/10: 100%|██████████| 225/225 [00:07<00:00, 29.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 6, Loss: 0.1636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 7/10: 100%|██████████| 225/225 [00:07<00:00, 28.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 7, Loss: 0.1629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 8/10: 100%|██████████| 225/225 [00:07<00:00, 29.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 8, Loss: 0.1607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 9/10: 100%|██████████| 225/225 [00:07<00:00, 29.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 9, Loss: 0.1632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[anime] Epoch 10/10: 100%|██████████| 225/225 [00:07<00:00, 29.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 anime - Epoch 10, Loss: 0.1623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 1/10: 100%|██████████| 188/188 [19:04<00:00,  6.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 1, Loss: 0.2233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 2/10: 100%|██████████| 188/188 [00:06<00:00, 29.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 2, Loss: 0.2096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 3/10: 100%|██████████| 188/188 [00:06<00:00, 28.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 3, Loss: 0.2079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 4/10: 100%|██████████| 188/188 [00:06<00:00, 29.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 4, Loss: 0.2051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 5/10: 100%|██████████| 188/188 [00:06<00:00, 28.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 5, Loss: 0.2036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 6/10: 100%|██████████| 188/188 [00:06<00:00, 30.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 6, Loss: 0.2011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 7/10: 100%|██████████| 188/188 [00:06<00:00, 29.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 7, Loss: 0.1993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 8/10: 100%|██████████| 188/188 [00:06<00:00, 29.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 8, Loss: 0.2008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 9/10: 100%|██████████| 188/188 [00:06<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 9, Loss: 0.1974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[painting] Epoch 10/10: 100%|██████████| 188/188 [00:06<00:00, 30.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 painting - Epoch 10, Loss: 0.1982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === INTERFAZ GRADIO FINAL ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Modelo de Estilo: VGG19 ---\n",
        "vgg = models.vgg19(pretrained=True).features[:29].eval().to(device)\n",
        "for p in vgg.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "def extract_style(img_tensor):\n",
        "    layers = [0, 5, 10, 19, 28]\n",
        "    features = []\n",
        "    x = img_tensor\n",
        "    for i, layer in enumerate(vgg):\n",
        "        x = layer(x)\n",
        "        if i in layers:\n",
        "            features.append(x.mean([2, 3]))\n",
        "    return torch.cat(features, dim=1)\n",
        "\n",
        "# --- Modelo de Identidad: FaceNet ---\n",
        "!pip install facenet-pytorch --quiet\n",
        "\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# --- Modelo de Estilización ---\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
        "        super().__init__()\n",
        "        if down:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        if use_dropout:\n",
        "            self.block.add_module(\"dropout\", nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class StylizerUNet(nn.Module):\n",
        "    def __init__(self, id_dim=512, style_dim=1472):\n",
        "        super().__init__()\n",
        "        self.emb_dim = id_dim + style_dim\n",
        "        self.embedding_expand = nn.Sequential(\n",
        "            nn.Linear(self.emb_dim, 512 * 4 * 4),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.enc1 = UNetBlock(1, 64, down=True)\n",
        "        self.enc2 = UNetBlock(64, 128, down=True)\n",
        "        self.enc3 = UNetBlock(128, 256, down=True)\n",
        "        self.enc4 = UNetBlock(256, 512, down=True)\n",
        "        self.enc5 = UNetBlock(512, 512, down=True)\n",
        "        self.enc6 = UNetBlock(512, 512, down=True)\n",
        "        self.enc7 = UNetBlock(512, 512, down=True)\n",
        "        self.middle = nn.Sequential(nn.Conv2d(1024, 512, 3, padding=1), nn.ReLU(True))\n",
        "        self.dec1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
        "        self.dec2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.dec3 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.dec4 = UNetBlock(1024, 512, down=False)\n",
        "        self.dec5 = UNetBlock(768, 256, down=False)\n",
        "        self.dec6 = UNetBlock(384, 128, down=False)\n",
        "        self.dec7 = UNetBlock(192, 64, down=False)\n",
        "        self.final = nn.Conv2d(65, 3, 3, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, pose_map, identity_vec, style_vec):\n",
        "        emb = torch.cat([identity_vec, style_vec], dim=1)\n",
        "        emb = self.embedding_expand(emb).view(-1, 512, 4, 4)\n",
        "        e1 = self.enc1(pose_map)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "        e6 = self.enc6(e5)\n",
        "        e7 = self.enc7(e6)\n",
        "        emb_up = F.interpolate(emb, size=e7.shape[2:])\n",
        "        bottleneck = self.middle(torch.cat([e7, emb_up], dim=1))\n",
        "        d1 = self.dec1(bottleneck)\n",
        "        d2 = self.dec2(torch.cat([d1, e6], dim=1))\n",
        "        d3 = self.dec3(torch.cat([d2, e5], dim=1))\n",
        "        d4 = self.dec4(torch.cat([d3, e4], dim=1))\n",
        "        d5 = self.dec5(torch.cat([d4, e3], dim=1))\n",
        "        d6 = self.dec6(torch.cat([d5, e2], dim=1))\n",
        "        d7 = self.dec7(torch.cat([d6, e1], dim=1))\n",
        "        pose_resized = F.interpolate(pose_map, size=d7.shape[2:])\n",
        "        out = self.final(torch.cat([d7, pose_resized], dim=1))\n",
        "        return self.tanh(out)\n",
        "\n",
        "# --- Transformaciones de entrada ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "def generate_image(pose_img, id_img, style_img, mode):\n",
        "    # Transformaciones\n",
        "    pose = transform(Image.fromarray(pose_img).convert(\"L\")).unsqueeze(0).to(device)\n",
        "    id_tensor = transform(Image.fromarray(id_img).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    style_tensor = transform(Image.fromarray(style_img).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "\n",
        "    # Extraer vectores\n",
        "    identity_vec = facenet(id_tensor)\n",
        "    style_vec = extract_style(style_tensor)\n",
        "\n",
        "    # 🔧 Normalización de vectores (muy importante)\n",
        "    identity_vec = F.normalize(identity_vec, dim=1)\n",
        "    style_vec = F.normalize(style_vec, dim=1)\n",
        "\n",
        "    # Cargar modelo\n",
        "    model = StylizerUNet().to(device)\n",
        "    if mode == \"anime\":\n",
        "        model.load_state_dict(torch.load(\"/content/model_anime_epoch10.pt\", map_location=device))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(\"/content/model_painting_epoch10.pt\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Inferencia\n",
        "    with torch.no_grad():\n",
        "        output = model(pose, identity_vec, style_vec)\n",
        "        output = output.squeeze(0).cpu().permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        return Image.fromarray((output * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "# === INTERFAZ ===\n",
        "gr.Interface(\n",
        "    fn=generate_image,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Mapa de Pose (PNG)\", type=\"numpy\"),\n",
        "        gr.Image(label=\"Imagen de Identidad (PNG)\", type=\"numpy\"),\n",
        "        gr.Image(label=\"Imagen de Estilo (PNG)\", type=\"numpy\"),\n",
        "        gr.Radio([\"anime\", \"painting\"], label=\"Modelo a Usar\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Imagen Estilizada\"),\n",
        "    title=\"StyleMatcher - Generador de Imágenes\",\n",
        "    description=\"Sube una imagen de pose, una de identidad y una de estilo. Elige el modelo (anime o pintura) para generar tu imagen estilizada.\"\n",
        ").launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "09480268e190491aa133ec8e73e0fa38",
            "7f6165fded4745be8ae42d24e5121383",
            "21c231277fc34c179ce2e698ba3a145d",
            "7ee7c0d6d29c4c7ab4e0c924c7834f30",
            "e99371b215424147a9d2b814cf2a4c7a",
            "c50302c146474ac28e4a0876dbdd0ca3",
            "3df51c915eef432595ab4177efa62d73",
            "276b070a1830497d8a9de5ed5bd20031",
            "aa544b37402346c4aadd06e0e138e1c2",
            "65944cfd13c34597baa84e0b86d9626a",
            "b6bddb72195b4ad4a6bac9d6a9d7dfcd"
          ]
        },
        "id": "PYRtE3ZEKljJ",
        "outputId": "bfa557a8-751d-4274-f4df-f12bc2c74ed7"
      },
      "id": "PYRtE3ZEKljJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 196MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09480268e190491aa133ec8e73e0fa38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c1e172c9699c9b13d8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c1e172c9699c9b13d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-61da5b47fce2>\", line 128, in generate_image\n",
            "    model.load_state_dict(torch.load(\"/content/model_anime_epoch10.pt\", map_location=device))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for StylizerUNet:\n",
            "\tMissing key(s) in state_dict: \"embedding_expand.0.weight\", \"embedding_expand.0.bias\", \"enc1.block.0.weight\", \"enc1.block.1.weight\", \"enc1.block.1.bias\", \"enc1.block.1.running_mean\", \"enc1.block.1.running_var\", \"enc2.block.0.weight\", \"enc2.block.1.weight\", \"enc2.block.1.bias\", \"enc2.block.1.running_mean\", \"enc2.block.1.running_var\", \"enc3.block.0.weight\", \"enc3.block.1.weight\", \"enc3.block.1.bias\", \"enc3.block.1.running_mean\", \"enc3.block.1.running_var\", \"enc4.block.0.weight\", \"enc4.block.1.weight\", \"enc4.block.1.bias\", \"enc4.block.1.running_mean\", \"enc4.block.1.running_var\", \"enc5.block.0.weight\", \"enc5.block.1.weight\", \"enc5.block.1.bias\", \"enc5.block.1.running_mean\", \"enc5.block.1.running_var\", \"enc6.block.0.weight\", \"enc6.block.1.weight\", \"enc6.block.1.bias\", \"enc6.block.1.running_mean\", \"enc6.block.1.running_var\", \"enc7.block.0.weight\", \"enc7.block.1.weight\", \"enc7.block.1.bias\", \"enc7.block.1.running_mean\", \"enc7.block.1.running_var\", \"middle.0.weight\", \"middle.0.bias\", \"dec1.block.0.weight\", \"dec1.block.1.weight\", \"dec1.block.1.bias\", \"dec1.block.1.running_mean\", \"dec1.block.1.running_var\", \"dec2.block.0.weight\", \"dec2.block.1.weight\", \"dec2.block.1.bias\", \"dec2.block.1.running_mean\", \"dec2.block.1.running_var\", \"dec3.block.0.weight\", \"dec3.block.1.weight\", \"dec3.block.1.bias\", \"dec3.block.1.running_mean\", \"dec3.block.1.running_var\", \"dec4.block.0.weight\", \"dec4.block.1.weight\", \"dec4.block.1.bias\", \"dec4.block.1.running_mean\", \"dec4.block.1.running_var\", \"dec5.block.0.weight\", \"dec5.block.1.weight\", \"dec5.block.1.bias\", \"dec5.block.1.running_mean\", \"dec5.block.1.running_var\", \"dec6.block.0.weight\", \"dec6.block.1.weight\", \"dec6.block.1.bias\", \"dec6.block.1.running_mean\", \"dec6.block.1.running_var\", \"dec7.block.0.weight\", \"dec7.block.1.weight\", \"dec7.block.1.bias\", \"dec7.block.1.running_mean\", \"dec7.block.1.running_var\", \"final.weight\", \"final.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"encoder.0.0.weight\", \"encoder.0.0.bias\", \"encoder.0.1.weight\", \"encoder.0.1.bias\", \"encoder.0.1.running_mean\", \"encoder.0.1.running_var\", \"encoder.0.1.num_batches_tracked\", \"encoder.1.0.weight\", \"encoder.1.0.bias\", \"encoder.1.1.weight\", \"encoder.1.1.bias\", \"encoder.1.1.running_mean\", \"encoder.1.1.running_var\", \"encoder.1.1.num_batches_tracked\", \"encoder.2.0.weight\", \"encoder.2.0.bias\", \"encoder.2.1.weight\", \"encoder.2.1.bias\", \"encoder.2.1.running_mean\", \"encoder.2.1.running_var\", \"encoder.2.1.num_batches_tracked\", \"encoder.3.0.weight\", \"encoder.3.0.bias\", \"encoder.3.1.weight\", \"encoder.3.1.bias\", \"encoder.3.1.running_mean\", \"encoder.3.1.running_var\", \"encoder.3.1.num_batches_tracked\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.6.weight\", \"decoder.6.bias\". \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d3c877aba6ac6c0958.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://8029bcf3006df0a913.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://295cfddaf23698c5a0.gradio.live\n",
            "Killing tunnel 127.0.0.1:7863 <> https://9894352b4c004cacdf.gradio.live\n",
            "Killing tunnel 127.0.0.1:7864 <> https://c1e172c9699c9b13d8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PRUEBA"
      ],
      "metadata": {
        "id": "5hgxl6NGXQha"
      },
      "id": "5hgxl6NGXQha"
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Instala face_recognition y dependencias necesarias en Colab\n",
        "!pip install -q face_recognition gradio opencv-python\n",
        "!apt-get install -y cmake libboost-all-dev\n",
        "!pip install -q dlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL5FvJytXQPO",
        "outputId": "2f9369c0-23d9-40bc-d3d7-3feaebdd2dea"
      },
      "id": "BL5FvJytXQPO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBCEMrxjdeu_",
        "outputId": "8951df8d-8e60-483b-f9a5-78d32734ff2c"
      },
      "id": "OBCEMrxjdeu_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anime_style_dir = \"/content/drive/MyDrive/StyleMatcher/dataset/train/anime\"\n",
        "painting_style_dir = \"/content/drive/MyDrive/StyleMatcher/dataset/train/painting\""
      ],
      "metadata": {
        "id": "LKu-1RkddgHg"
      },
      "id": "LKu-1RkddgHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "\n",
        "# Función para obtener imágenes de estilo\n",
        "def get_style_images(style_type):\n",
        "    folder = anime_style_dir if style_type == \"anime\" else painting_style_dir\n",
        "    return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "\n",
        "# Función principal de combinación\n",
        "def combine_face_with_style(user_image, style_path):\n",
        "    user_np = np.array(user_image.convert(\"RGB\"))\n",
        "    faces = face_recognition.face_locations(user_np)\n",
        "    if not faces:\n",
        "        return user_image, Image.open(style_path), user_image\n",
        "\n",
        "    top, right, bottom, left = faces[0]\n",
        "    face_crop = user_np[top:bottom, left:right]\n",
        "    face_img = Image.fromarray(face_crop).resize((256, 256))\n",
        "    style_img = Image.open(style_path).convert(\"RGB\").resize((256, 256))\n",
        "    alpha = 0.5\n",
        "    blended = Image.blend(face_img, style_img, alpha)\n",
        "    return user_image, style_img, blended"
      ],
      "metadata": {
        "id": "AXqWgY1XdnDb"
      },
      "id": "AXqWgY1XdnDb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "style_list = get_style_images(\"anime\")\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎨 Fusionador de Rostro con Estilo Anime o Pintura\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(label=\"1️⃣ Imagen del Usuario\", type=\"pil\")\n",
        "        style_selector = gr.Dropdown(label=\"2️⃣ Estilo a aplicar\", choices=style_list, value=style_list[0])\n",
        "        style_type = gr.Radio([\"anime\", \"painting\"], label=\"🎭 Tipo de estilo\", value=\"anime\")\n",
        "\n",
        "    run_button = gr.Button(\"✨ Aplicar Estilo\")\n",
        "\n",
        "    with gr.Row():\n",
        "        original_output = gr.Image(label=\"🧍 Imagen Original\")\n",
        "        style_output = gr.Image(label=\"🎨 Estilo Elegido\")\n",
        "        result_output = gr.Image(label=\"🧬 Resultado Fusionado\")\n",
        "\n",
        "    def update_styles(tipo):\n",
        "        new_list = get_style_images(tipo)\n",
        "        return gr.update(choices=new_list, value=new_list[0])\n",
        "\n",
        "    style_type.change(fn=update_styles, inputs=style_type, outputs=style_selector)\n",
        "    run_button.click(fn=combine_face_with_style, inputs=[image_input, style_selector],\n",
        "                     outputs=[original_output, style_output, result_output])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "D3SdKK2Zdocq",
        "outputId": "a50d890d-4bb8-4c23-de42-dd82e9769b18"
      },
      "id": "D3SdKK2Zdocq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e49eda27ec84dab8ee.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e49eda27ec84dab8ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRUEBA 2"
      ],
      "metadata": {
        "id": "2L1zQAVjgJeJ"
      },
      "id": "2L1zQAVjgJeJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nagadomi/animeface-2009.git\n",
        "%cd animeface-2009\n",
        "!sudo apt-get install -y libopencv-dev\n",
        "!make\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnMHNkHQgJF5",
        "outputId": "7cd46080-6710-4759-fb67-5f2cd4a2b644"
      },
      "id": "PnMHNkHQgJF5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'animeface-2009'...\n",
            "remote: Enumerating objects: 2961, done.\u001b[K\n",
            "remote: Total 2961 (delta 0), reused 0 (delta 0), pack-reused 2961 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2961/2961), 9.16 MiB | 18.29 MiB/s, done.\n",
            "Resolving deltas: 100% (1976/1976), done.\n",
            "/content/animeface-2009\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopencv-dev is already the newest version (4.5.4+dfsg-9ubuntu4+jammy1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "make: *** No targets specified and no makefile found.  Stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "\n",
        "def get_style_images(style_type):\n",
        "    folder = anime_style_dir if style_type == \"anime\" else painting_style_dir\n",
        "    return sorted([\n",
        "        os.path.join(folder, f) for f in os.listdir(folder)\n",
        "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    ])\n",
        "\n",
        "def fuse_face_into_style(user_image, style_path):\n",
        "    # Convertir imágenes\n",
        "    user_np = np.array(user_image.convert(\"RGB\"))\n",
        "    style_np = cv2.cvtColor(cv2.imread(style_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detectar rostro del usuario\n",
        "    user_faces = face_recognition.face_locations(user_np)\n",
        "    if not user_faces:\n",
        "        return user_image, Image.open(style_path), user_image\n",
        "\n",
        "    u_top, u_right, u_bottom, u_left = user_faces[0]\n",
        "    face_crop = user_np[u_top:u_bottom, u_left:u_right]\n",
        "    face_resized = cv2.resize(face_crop, (128, 128))\n",
        "\n",
        "    # Detectar rostro en la imagen de estilo\n",
        "    style_faces = face_recognition.face_locations(style_np)\n",
        "    if not style_faces:\n",
        "        return user_image, Image.open(style_path), user_image\n",
        "\n",
        "    s_top, s_right, s_bottom, s_left = style_faces[0]\n",
        "    center = ((s_left + s_right) // 2, (s_top + s_bottom) // 2)\n",
        "\n",
        "    # Crear máscara blanca del rostro\n",
        "    mask = 255 * np.ones(face_resized.shape, face_resized.dtype)\n",
        "\n",
        "    # Fusión con seamlessClone\n",
        "    fused = cv2.seamlessClone(face_resized, style_np, mask, center, cv2.NORMAL_CLONE)\n",
        "\n",
        "    return Image.fromarray(user_np), Image.open(style_path), Image.fromarray(fused)\n"
      ],
      "metadata": {
        "id": "ZOm79pNIgOHx"
      },
      "id": "ZOm79pNIgOHx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Cargar lista inicial de estilos (anime por defecto)\n",
        "style_list = get_style_images(\"anime\")\n",
        "\n",
        "# Interfaz Gradio completa\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎭 Fusión de Rostro con Estilo Artístico\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(label=\"📸 Imagen del Usuario\", type=\"pil\")\n",
        "        style_type = gr.Radio([\"anime\", \"painting\"], label=\"🎨 Tipo de Estilo\", value=\"anime\")\n",
        "        style_selector = gr.Dropdown(choices=style_list, label=\"🖼️ Imagen de Estilo\", value=style_list[0] if style_list else None)\n",
        "\n",
        "    run_btn = gr.Button(\"✨ Fusionar\")\n",
        "\n",
        "    with gr.Row():\n",
        "        out_orig = gr.Image(label=\"1️⃣ Imagen Original\")\n",
        "        out_style = gr.Image(label=\"2️⃣ Imagen de Estilo\")\n",
        "        out_result = gr.Image(label=\"3️⃣ Resultado Final\")\n",
        "\n",
        "    # Cambiar lista de estilos al cambiar tipo\n",
        "    def actualizar_estilos(tipo):\n",
        "        opciones = get_style_images(tipo)\n",
        "        return gr.update(choices=opciones, value=opciones[0] if opciones else None)\n",
        "\n",
        "    style_type.change(fn=actualizar_estilos, inputs=style_type, outputs=style_selector)\n",
        "\n",
        "    run_btn.click(fn=fuse_face_into_style, inputs=[image_input, style_selector],\n",
        "                  outputs=[out_orig, out_style, out_result])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "r9Nfa5trgXfJ",
        "outputId": "17326810-39a6-48c9-b3d9-0980be22edc9"
      },
      "id": "r9Nfa5trgXfJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://079e05cfc187fba689.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://079e05cfc187fba689.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformaciones\n",
        "def image_loader(img, max_size=400, shape=None):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert(\"RGB\")\n",
        "    if max(img.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(img.size)\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])])\n",
        "    image = transform(img).unsqueeze(0)\n",
        "    return image.to(device)\n",
        "\n",
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().squeeze(0)\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return image\n",
        "\n",
        "# Modelo VGG19\n",
        "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Extracción de características\n",
        "def get_features(image, model, layers=None):\n",
        "    if layers is None:\n",
        "        layers = {\n",
        "            '0': 'conv1_1',\n",
        "            '5': 'conv2_1',\n",
        "            '10': 'conv3_1',\n",
        "            '19': 'conv4_1',\n",
        "            '21': 'conv4_2',\n",
        "            '28': 'conv5_1'\n",
        "        }\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    _, d, h, w = tensor.size()\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    return torch.mm(tensor, tensor.t())\n",
        "\n",
        "# Función principal de inferencia\n",
        "def style_transfer(content_img, style_img):\n",
        "    content = image_loader(content_img)\n",
        "    style = image_loader(style_img, shape=content.shape[-2:])\n",
        "\n",
        "    content_features = get_features(content, vgg)\n",
        "    style_features = get_features(style, vgg)\n",
        "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    target = content.clone().requires_grad_(True).to(device)\n",
        "    optimizer = optim.Adam([target], lr=0.003)\n",
        "\n",
        "    style_weights = {\n",
        "        'conv1_1': 1.0,\n",
        "        'conv2_1': 0.75,\n",
        "        'conv3_1': 0.2,\n",
        "        'conv4_1': 0.2,\n",
        "        'conv5_1': 0.2\n",
        "    }\n",
        "\n",
        "    content_weight = 1e4\n",
        "    style_weight = 1e2\n",
        "\n",
        "    steps = 200\n",
        "    for i in range(steps):\n",
        "        target_features = get_features(target, vgg)\n",
        "        content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
        "\n",
        "        style_loss = 0\n",
        "        for layer in style_weights:\n",
        "            target_feature = target_features[layer]\n",
        "            target_gram = gram_matrix(target_feature)\n",
        "            style_gram = style_grams[layer]\n",
        "            layer_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "            style_loss += layer_loss / (target_feature.shape[1] ** 2)\n",
        "\n",
        "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return im_convert(target)\n",
        "\n",
        "# Interfaz Gradio\n",
        "gr.Interface(\n",
        "    fn=style_transfer,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Imagen base (contenido)\", type=\"pil\"),\n",
        "        gr.Image(label=\"Imagen estilo\", type=\"pil\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Resultado estilizado\"),\n",
        "    title=\"🎨 Style Transfer con VGG19\",\n",
        "    description=\"Generación de imagen estilizada combinando contenido y estilo. No necesita entrenamiento.\",\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "LA_F4XOAhJYx"
      },
      "id": "LA_F4XOAhJYx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09480268e190491aa133ec8e73e0fa38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f6165fded4745be8ae42d24e5121383",
              "IPY_MODEL_21c231277fc34c179ce2e698ba3a145d",
              "IPY_MODEL_7ee7c0d6d29c4c7ab4e0c924c7834f30"
            ],
            "layout": "IPY_MODEL_e99371b215424147a9d2b814cf2a4c7a"
          }
        },
        "7f6165fded4745be8ae42d24e5121383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c50302c146474ac28e4a0876dbdd0ca3",
            "placeholder": "​",
            "style": "IPY_MODEL_3df51c915eef432595ab4177efa62d73",
            "value": "100%"
          }
        },
        "21c231277fc34c179ce2e698ba3a145d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_276b070a1830497d8a9de5ed5bd20031",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa544b37402346c4aadd06e0e138e1c2",
            "value": 111898327
          }
        },
        "7ee7c0d6d29c4c7ab4e0c924c7834f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65944cfd13c34597baa84e0b86d9626a",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bddb72195b4ad4a6bac9d6a9d7dfcd",
            "value": " 107M/107M [00:00&lt;00:00, 365MB/s]"
          }
        },
        "e99371b215424147a9d2b814cf2a4c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50302c146474ac28e4a0876dbdd0ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df51c915eef432595ab4177efa62d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "276b070a1830497d8a9de5ed5bd20031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa544b37402346c4aadd06e0e138e1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65944cfd13c34597baa84e0b86d9626a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bddb72195b4ad4a6bac9d6a9d7dfcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}